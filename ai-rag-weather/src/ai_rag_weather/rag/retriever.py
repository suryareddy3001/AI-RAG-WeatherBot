from __future__ import annotations

from typing import Any, Dict, List, Optional
from langchain_core.messages import SystemMessage, HumanMessage

"""Retrieval-Augmented Generation (RAG) retriever for RAGChain-WeatherBot.

This module provides a RAGRetriever class that performs vector-based retrieval
from a vector database and generates concise answers using a language model.
It supports embedding queries, retrieving relevant contexts, and summarizing
results based on the retrieved contexts.
"""

class RAGRetriever:
    """Retriever for performing RAG operations with a vector database and LLM.

    Handles query embedding, vector search, and answer generation using retrieved contexts.
    """
    def __init__(
        self,
        vectordb: Any,
        embeddings: Any,
        llm: Any,
        top_k: int = 5,
        score_threshold: Optional[float] = None,
    ):
        """Initialize the RAG retriever.

        Args:
            vectordb: Vector database instance for similarity search.
            embeddings: Embeddings provider for converting text to vectors.
            llm: Language model for generating answers.
            top_k: Number of top results to retrieve (default: 5).
            score_threshold: Minimum score threshold for retrieved results (default: None).
        """
        self.vectordb = vectordb
        self.embeddings = embeddings
        self.llm = llm
        self.top_k = top_k
        self.score_threshold = score_threshold

    def _embed(self, text: str) -> List[float]:
        """Embed a text query into a vector.

        Args:
            text: The input text to embed.

        Returns:
            A list of floats representing the embedded vector.
        """
        vec = self.embeddings.embed_query(text)
        return vec.tolist() if hasattr(vec, "tolist") else vec

    def retrieve(self, query: str) -> Dict[str, Any]:
        """Retrieve relevant contexts for a query from the vector database.

        Args:
            query: The input query string.

        Returns:
            A dictionary containing the query and a list of contexts, where each context
            includes id, score, page, and text.
        """
        query_vec = self._embed(query)
        hits = self.vectordb.search(
            query_vector=query_vec,
            top_k=self.top_k,
            score_threshold=self.score_threshold,
        )

        contexts: List[Dict[str, Any]] = []
        for h in hits:
            payload = h.get("payload", {}) or {}
            contexts.append(
                {
                    "id": h.get("id"),
                    "score": float(h.get("score", 0.0) or 0.0),
                    "page": int(payload.get("page", -1)),
                    "text": payload.get("text") or "",
                }
            )
        return {"query": query, "contexts": contexts}

    def summarize(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """Generate a concise answer using retrieved contexts.

        Args:
            query: The input query string.
            contexts: List of retrieved contexts, each containing id, score, page, and text.

        Returns:
            A concise answer generated by the language model based on the query and contexts.
        """
        if not contexts:
            joined = "(no context found)"
        else:
            joined = "\n\n".join(
                f"[p.{c.get('page', -1)} | score={round(c.get('score', 0.0), 4)}]\n{c.get('text','')}"
                for c in contexts
            )

        system = SystemMessage(
            content=(
                "You are a helpful assistant. Use the provided context excerpts to answer the user's question. "
                "If the context is insufficient, say so briefly."
            )
        )
        user = HumanMessage(
            content=f"Question: {query}\n\nContext:\n{joined}\n\nWrite a concise answer."
        )

        ai_msg = self.llm.invoke([system, user])
        return getattr(ai_msg, "content", str(ai_msg))
